{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meridor6919/BeeClassification/blob/master/doc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preview of CNN based on bee_classification\n",
        "-------------------\n",
        "\n",
        "## Spis treści\n",
        "1. [Wstęp](#1)\n",
        "    1. [Cel projektu](#1.1)\n",
        "    2. [Czym jest CNN?](#1.2)\n",
        "    3. [Z czego składa się bee_dataset?](#1.3)\n",
        "2. [Model](#2)\n",
        "    1. [Specyfika modelu](#2.1)\n",
        "    2. [Część eksperymentalna](#2.2)\n",
        "        1. [Architektura](#2.2.1)\n",
        "        2. [Porównanie jakości klasyfikacji](#2.2.2)\n",
        "    3. [Użytkowanie](#2.3)\n",
        "        1. [Przygotowanie danych](#2.3.1)\n",
        "        2. [Opis input-output](#2.3.2)\n",
        "3. [Contributing](#3)\n",
        "4. [Źródła](#4)\n",
        "\n",
        "\n",
        "<a name=\"1\"></a>\n",
        "# Wstęp\n",
        "-------------------\n",
        "\n",
        "<a name=\"1.1\"></a>\n",
        "## Cel projektu\n",
        "-------------------\n",
        "Projekt ma na celu przedstawienie funkcjonowania konwolucyjnych sieci neuronowych przy pomocy TensorFlow Keras oraz podstawowej bazy danych pakietu tensorflow_datasets.\n",
        "\n",
        "<a name=\"1.2\"></a>\n",
        "## Czym jest CNN?\n",
        "-------------------\n",
        "Aby odpowiedzieć na pytanie, czym jest **Konwolucyjna Sieć Neuronowa** (ang. Convolutional Neural Network, inaczej CNN), pierw musimy przybliżyć pojęcie sztucznej sieci neuronowej.<br/>\n",
        "Sieci neuronowe są w ogólności modelem obliczeniowym inspirowanym strukturą i funkcją ludzkiego mózgu. Składają się z dużej liczby prostych jednostek przetwarzających, zwanych sztucznymi neuronami, które są zorganizowane w warstwy i połączone ze sobą za pomocą wag i odchyleń, by jak najlepiej rozpoznać podstawowe relacje w zbiorze danych.\n",
        "\n",
        "Rozłóżmy sieć neuronową na jej podstawowe bloki konstrukcyjne:\n",
        "1. **Tensor** jest uogólnieniem macierzy na dowolną liczbę wymiarów. Są one używane do reprezentowania danych w sieci neuronowej - prawie zawsze są to dane liczbowe.\n",
        "![tensor](https://drive.google.com/uc?id=1ZvNacNdu_RG0jr5U2PrwVoQsRg07Jzjb)\n",
        "\n",
        "2. **Neuron** można przedstawić jako funkcję, która przyjmuje wiele wejść i daje jedno wyjście.\n",
        "![sztuczny neuron](https://drive.google.com/uc?id=1k3OCAmfXWrhs_lv-ER75e0I9OOaUqaDQ)\n",
        "\n",
        "3. **Warstwa** to zbiór neuronów o tym samym działaniu, z tymi samymi hiperparametrami.\n",
        "\n",
        "4. **Wagi filtra** (inaczej wagi jądra lub maski), choć unikalne dla każdego neuronu, są dostrajane w fazie treningu w celu optymalizacji wydajności sieci na danym zadaniu.\n",
        "\n",
        "5. **Odchylenie** (ang. bias) jest skalarem, który pozwala budować model liniowy, który nie jest ustalony na początku i ma możliwość przeszukiwania przestrzeni rozwiązań. Odchylenie jest uczone wraz z wagami filtra.\n",
        "![sztuczny neuron](https://drive.google.com/uc?id=1_XR0_czVA84L2-GpoVlHluIfKWvuYN_9)\n",
        "\n",
        "**Czym różni się CNN od klasycznego modelu uczenia maszynowego?**\n",
        "\n",
        "CNN wykorzystuje specjalny typ warstwy - warstwę konwolucyjną, która jest dobrze przygotowana do uczenia się z obrazów i danych obrazopodobnych. Sieć taka może być wykorzystana do zadań związanych z widzeniem komputerowym, takich jak przetwarzanie obrazów, klasyfikacja, segmentacja i wykrywanie obiektów.<br/><br/>\n",
        "\n",
        "**Czy zajmują sie poszczególne warstwy sieci?**\n",
        "\n",
        "**Warstwa wejściowa** (ang. Input Layer) - pierwsza warstwa sieci, ma za zadanie przechwycenie sygnału i przekazanie go do kolejnych warstw sieci (zwanych warstwami ukrytymi).\n",
        "\n",
        "**Warstwa konwolucyjna** (ang. Convolutional Layer) - stosuje filtry do danych wejściowych i uczy się rozpoznawania cech. Początkowe warstwy konwolucyjne wydobywają ogólne lub niskopoziomowe cechy, takie jak linie i krawędzie, a kolejne warstwy uczą się subtelniejszych szczegółów lub cech wysokopoziomowych. \n",
        "![warstwa konwolucyjna](https://drive.google.com/uc?id=1-qrBQ0ynVIu5X8Nr9ZaQmtFX_OKJeD6j)\n",
        "\n",
        "**Funkcje aktywacyjne** (ang. Activation Functions) - nakładane elementarnie na dane wejściowe po nałożeniu wag. Wprowadzają one nieliniowość do sieci, pozwalając modelowi na uczenie się bardziej złożonych funkcji. Niektóre z najbardziej popularnych funkcji aktywacji obejmują:\n",
        "![sztuczny neuron](https://drive.google.com/uc?id=1E23OOrObsAkbnJ27MG_pRQnDNZnB2sA6)\n",
        "\n",
        "**Warstwa łącząca** (ang. Pooling Layer) - jest używana do zmniejszania rozmiaru objętości wejściowej, aby zmniejszyć moc obliczeniową wymaganą do przetworzenia obrazu, nie tracąc przy tym właściwości obrazu.\n",
        "\n",
        "**Warstwa dropout** (ang. Dropout Layer) - losowo wyrzuca pewne neurony podczas treningu by zapobiec nadmiernemu dopasowaniu.\n",
        "\n",
        "**Warstwa w pełni połączona** (ang. Fully Connected Layer albo Dense Layer) pobiera wyjście z końcowej warstwy łączącej i wykorzystują je do tworzenia predykcji przy użyciu standardowych technik sieci neuronowych. Jest zazwyczaj ostatnią warstwą przed warstwą wyjściową.\n",
        "\n",
        "**Funkcja straty** (ang. Loss Function) - pobiera ostateczne wyjście sieci, wytworzone przez warstwę w pełni połączoną, i porównuje je z wyjściem prawdziwym. Na tej podstawie obliczany jest błąd który jest wykorzystywany do \n",
        "aktualizacji wag i odchyleń.\n",
        "\n",
        "**Warstwa wyjściowa** (ang. Output Layer) - ostatnia warstw sieci, w której neuronach formowane są sygnały wyjściowe.<br/><br/>\n",
        "\n",
        "**Co może doprowadzić do nadmiernego dopasowanie danych?**\n",
        "\n",
        "**Nadmierne Dopasowanie** (ang. Overfitting) to zjawisko, które występuje, gdy model jest zbyt złożony i jest w stanie dopasować się do szumu lub losowych zmian w danych treningowych, a nie do leżących u jego podstaw wzorców.<br/>\n",
        "Istnieje kilka czynników, które mogą do tego doprowadzić:\n",
        "  * Niewystarczająca ilość danych\n",
        "\n",
        "  * Duża złożoność modelu: Model o zbyt wielu parametrach lub wysokim stopniu swobody.\n",
        "\n",
        "  * Brak regularności: Regularność jest techniką dodającą termin kary do funkcji straty, która zniechęca do dużych wag, zapobiegając przy tym dopasowaniu do szumu w danych treningowych.\n",
        "\n",
        "  * Wyciek danych: Wyciek danych występuje, gdy informacje z zestawu testowego są używane do informowania modelu podczas szkolenia. Model będzie uzyskiwał dobre wyniki na zestawie testowym, ale nie będzie dobrze generalizować na nowe przykłady.\n",
        "\n",
        "<a name=\"1.3\"></a>\n",
        "## Z czego składa się bee_dataset?\n",
        "-------------------\n",
        "[Bee_dataset](https://www.tensorflow.org/datasets/catalog/bee_dataset) \n",
        "jest zbiorem obrazów oraz odpowiadających im etykiet stworzonym do uczenia algorytmu klasyfikacji obrazów do czterech odrębnych kategorii:\n",
        "  * czy pszczoła jest zainfekowana dręczem pszczelim\n",
        "  * czy  pszczoła przenosi pyłek\n",
        "  * czy jest to osa\n",
        "  * czy pszczoła chłodzi ul machając skrzydłami\n",
        "\n",
        "Powyższe dane zapisane są jako parametry:<br/>\n",
        "```{'cooling_output': 0.0, 'pollen_output': 0.0, 'varroa_output': 0.0, 'wasps_output': 0.0}```<br/>\n",
        "Jeśli dana cecha występuje przypisywane jest 1, jeśli nie 0.\n",
        "\n",
        "![bees with labels](https://drive.google.com/uc?id=1xVVYghka5JwmhWxaQA-FBMjuvzPkadr4)\n",
        "\n",
        "Cała baza danych składa się z 7490 przykładów i jest podzielona wyłącznie na zbiór treningowy:<br/>\n",
        "```splits={'train': <SplitInfo num_examples=7490, num_shards=1>,}```<br/>\n",
        "Każdy obraz ma wymiar (300, 150, 3) tzn. 300px wysokości, 150px szerokości, 3 składowe odpowiadające kolorom:<br/>\n",
        "```Image(shape=(300, 150, 3), dtype=uint8)```\n",
        "\n",
        "W bazie danych znajdują się również obrazy, które algorytm uczy się klasyfikować jako same zera (ponieważ są niepoprawne lub nie należą do żadnej kategorii) oraz obrazy spełniające więcej niżeli jedną kategorię:\n",
        "\n",
        "![bee flawed and double](https://drive.google.com/uc?id=1-zN-SQ06V0RfPXlsdB9whXer7hoLPDn0)\n",
        "\n",
        "Wszystkie zdjęcia zostały wykonane z góry, na zielonym tle, w tych samych odległościach pszczół od kamery. Odwrócono je tak, by głowa lub tułów znajdowały się na górze zdjęcia.<br/><br/>\n",
        "\n",
        "**Jakiej obróbce poddaliśmy naszą bazę danych?**\n",
        "\n",
        "Wczytujemy nasz dataset.\n",
        "\n",
        "```\n",
        "dataset = tfds.load('bee_dataset', split=['train'], as_supervised=True)\n",
        "```\n",
        "\n",
        "Ustalamy przedziały zakresów, według których będziemy dzielić nasz dataset na zbiór treningowy, walidacyjny i testowy w proporcjach:\n",
        "\n",
        "```\n",
        "splitSize = (0.0, 0.765, 0.9, 1.0)\n",
        "```\n",
        "\n",
        "Konwertujemy dataset na tablicę Numpy, a następnie dokonujemy pionowej konkatenacji i dzielimy ją według wcześniej ustalonych zakresów ```splitSize```.\n",
        "\n",
        "```\n",
        "array = np.vstack(tfds.as_numpy(dataset[0]))\n",
        "ranges = list([int(array.shape[0] * x) for x in splitSize])\n",
        "```\n",
        "\n",
        "Definiujemy funkcję pomocniczą ```getDataset```, która porcjuje dataset.\n",
        "\n",
        "```\n",
        "def getDataset(ranges, idx):\n",
        "    return (\n",
        "        np.array(list(map(lambda x: x[0][:, :, :], array[ranges[idx]: ranges[idx + 1]]))),\n",
        "        np.array(list(map(lambda x: x[1]['varroa_output'], array[ranges[idx]: ranges[idx + 1]]))),\n",
        "        np.array(list(map(lambda x: x[1]['pollen_output'], array[ranges[idx]: ranges[idx + 1]]))),\n",
        "        np.array(list(map(lambda x: x[1]['wasps_output'], array[ranges[idx]: ranges[idx + 1]]))),\n",
        "        np.array(list(map(lambda x: x[1]['cooling_output'], array[ranges[idx]: ranges[idx + 1]])))\n",
        "    )\n",
        "``` \n",
        "\n",
        "Metodą pomocniczą ```getDataset``` pobieramy porcje danych zgodne z zakresami.\n",
        "\n",
        "```\n",
        "train_images, train_labels1, train_labels2, train_labels3, train_labels4 = getDataset(ranges, 0)\n",
        "validation_images, validation_labels1, validation_labels2, validation_labels3, validation_labels4 = getDataset(ranges, 1)\n",
        "test_images, test_labels1, test_labels2, test_labels3, test_labels4 = getDataset(ranges, 2)\n",
        "```\n",
        "\n",
        "Wszystkie etykiety scalane są w jedną strukturę.\n",
        "\n",
        "```\n",
        "train_labels = np.dstack((train_labels1, train_labels2, train_labels3,train_labels4)).squeeze()\n",
        "validation_labels = np.dstack((validation_labels1,validation_labels2, validation_labels3, validation_labels4)).squeeze()\n",
        "test_labels = np.dstack((test_labels1,test_labels2,test_labels3, test_labels4)).squeeze()\n",
        "```\n",
        "<br/>\n",
        "\n",
        "**Wyróżnione kroki w przetwarzaniu danych**\n",
        "<br/>\n",
        "\n",
        "  **Normalizacja** (ang. Normalisation) - najważniejszy krok w części wstępnego przetwarzania. Polega ona na przeskalowaniu wartości pikseli tak, aby mieściły się one w określonym zakresie(najczęściej 0-1). Przyspiesza proces uczenia i osłabia zależność od inicjalizacji.\n",
        "\n",
        "  **Standaryzacja** (ang. Standardisation) - przekształcenie danych numerycznych w taki sposób, aby średnia wartości danych była równa 0, a odchylenie standardowe 1.\n",
        "\n",
        "  **Kodowanie etykiety** (ang. Label Encoding) - przekształcenie zmiennych kategorycznych do wektorów o wartości 0 lub 1.\n",
        "\n",
        "  **Morfologiczne przekształcenia obrazów** (ang. Morphological Image Transformations) - modyfikacje obejmujące kształt i formę obrazów. Zaliczają się do nich:\n",
        "  * Tworzenie obrazu binarnego\n",
        "  * Erozja (kurczenie jasnych regionów i powiększanie ciemnych)\n",
        "  * Dylatacja (kurczenie ciemnych regionów i powiększanie jasnych)\n",
        "  * Otwarcie (dylatacja poprzedzona erozją, proces usuwa małe jasne plamy tzw. sól)\n",
        "  * Zamknięcie (erozja poprzedzona dylatacją, proces usuwa małe czarne plamy tzw. pieprz)\n",
        "\n",
        "**Rozszerzenie danych** (ang. Data Augmentation) - zestaw technik, które sztucznie generują nowe dane z istniejących danych. Zaliczają się do nich: obrót, rotacja, losowe wycinki, zmiana koloru, dodanie szumu, utrata informacji, zmiana kontrastu.<br/><br/>\n",
        "\n",
        "\n",
        "<a name=\"2\"></a>\n",
        "# Model\n",
        "-------------------\n",
        "\n",
        "<a name=\"2.1\"></a>\n",
        "## Specyfika modelu\n",
        "-------------------\n",
        "\n",
        "<a name=\"2.2\"></a>\n",
        "## Część eksperymentalna\n",
        "-------------------\n",
        "\n",
        "<a name=\"2.2.1\"></a>\n",
        "### Architektura\n",
        "```\n",
        "\n",
        "model= tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(filters=10, \n",
        "                         kernel_size=3, \n",
        "                         activation=\"relu\", \n",
        "                         input_shape=(300, 150, 3)), \n",
        "  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n",
        "  tf.keras.layers.MaxPool2D(pool_size=2, \n",
        "                            padding=\"valid\"), \n",
        "  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n",
        "  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"), \n",
        "\n",
        "  tf.keras.layers.MaxPool2D(2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(4, activation=\"sigmoid\") \n",
        "])\n",
        "```\n",
        "![layers](https://drive.google.com/uc?id=1H5YY8okmA36AcKFIIDX5P6P2jneaeAym)\n",
        "\n",
        "<a name=\"2.2.2\"></a>\n",
        "### Porównanie jakości klasyfikacji\n",
        "-------------------\n",
        "<a name=\"2.2.3\"></a>\n",
        "### Trening\n",
        "Sieć była trenowana przy użyciu GPU Tesla T4, co znacznie przyspieszyło cały proces i pozwoliło osiągnąć wydajność około 41ms/step (~7s/epoch). \n",
        "```\n",
        "Sat Jan 21 14:21:47 2023       \n",
        "+-----------------------------------------------------------------------------+\n",
        "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
        "|-------------------------------+----------------------+----------------------+\n",
        "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
        "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
        "|                               |                      |               MIG M. |\n",
        "|===============================+======================+======================|\n",
        "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
        "| N/A   59C    P0    26W /  70W |      0MiB / 15109MiB |      4%      Default |\n",
        "|                               |                      |                  N/A |\n",
        "+-------------------------------+----------------------+----------------------+\n",
        "```\n",
        "```\n",
        "Epoch 20/20\n",
        "180/180 [==============================] - 7s 41ms/step - loss: 0.0058 - all: 0.9961 - varroa: 0.9884 - pollen: 0.9964 - wasps: 1.0000 - cooling: 0.9998 - val_loss: 0.2819 - val_all: 0.9453 - val_varroa: 0.8906 - val_pollen: 0.9154 - val_wasps: 0.9971 - val_cooling: 0.9779\n",
        "```\n",
        "-------------------\n",
        "<a name=\"2.3\"></a>\n",
        "## Użytkowanie\n",
        "-------------------\n",
        "\n",
        "<a name=\"2.3.1\"></a>\n",
        "### Przygotowanie danych\n",
        "-------------------\n",
        "<a name=\"2.3.2\"></a>\n",
        "### Opis input-output\n",
        "-------------------\n",
        "\n",
        "<a name=\"3\"></a>\n",
        "# Contributing\n",
        "-------------------\n",
        "\n",
        "<a name=\"4\"></a>\n",
        "# Źródła\n",
        "-------------------\n",
        "1. [Deep Learning with Python by François Chollet, version 6, 2017](https://www.manning.com/books/deep-learning-with-python)\n",
        "\n",
        "2. [Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow by Aurélien Géron, 2nd edition, 2019](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n",
        "\n",
        "3. [TensorFlow Keras Documentation](https://www.tensorflow.org/api_docs/python/tf/keras)\n",
        "\n",
        "4. [CNN Explainer](https://github.com/poloclub/cnn-explainer)\n",
        "\n",
        "5. [Deep Learning cheatsheets for Stanford's CS 230](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks)\n",
        "\n",
        "6. [Jaki jest związek pomiędzy sieciami CNN i głębokim uczeniem?](https://www.intel.pl/content/www/pl/pl/internet-of-things/computer-vision/convolutional-neural-networks.html)\n",
        "\n",
        "7. Wykład \"Uczenie maszynowe w Pythonie\" dr Macieja Ślęczki\n",
        "\n",
        "8. [Wikipedia](http://wikipedia.org)\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "![zdjecie](https://drive.google.com/uc?id=16xYDkCsPLDC8VhVVwB5bq3rpxdC-fnYM)\n",
        "\n",
        "\n",
        "<style type=\"text/css\">\n",
        "    ol { list-style-type: decimal; }\n",
        "</style>\n",
        "\n",
        "\n",
        "- contributing\n",
        "- użytkowanie"
      ],
      "metadata": {
        "id": "AKKD9l7g7Ilm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H5wVtwBFLIqA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}